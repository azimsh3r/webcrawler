# ğŸŒ Distributed Web Crawler

## ğŸš€ Overview
This is a **high-performance distributed web crawler** built using **Java, Spring Boot, Kafka, Redis, and Elasticsearch**. It efficiently fetches, parses, deduplicates, and indexes web pages for fast searching.

## ğŸ”¥ Features
âœ… **Microservices Architecture** â€“ Independently scalable services
âœ… **Asynchronous Processing** â€“ Uses Kafka & Redis for efficiency
âœ… **Elasticsearch Integration** â€“ Enables full-text search on crawled pages
âœ… **Scalability & Fault Tolerance** â€“ Designed to handle large-scale crawling
âœ… **Deduplication System** â€“ Prevents duplicate URL processing

## ğŸ—ï¸ Architecture
```
   [Internet] â†’ [Crawler] â†’ [Parser] â†’ [URL Extractor] â†’ [Deduplicator] â†’ [Queue] â†’ [Crawlers]
                                         â†“
                                      [Elasticsearch (Search Index)]
```
### **ğŸ”¹ Service Responsibilities**
- **Crawler Service** â€“ Fetches web pages from the internet.
- **HTML Parser Service** â€“ Extracts text & metadata from raw HTML.
- **URL Extractor Service** â€“ Extracts and sends unique links for crawling.
- **Deduplicator Service** â€“ Removes duplicate URLs using Redis/Bloom Filters.
- **URL Queue Service** â€“ Manages the queue of URLs to be crawled.
- **Search Indexer (Elasticsearch)** â€“ Enables fast keyword-based searching.

## âš™ï¸ Tech Stack
- **Backend:** Java, Spring Boot
- **Message Queue:** Kafka / RabbitMQ
- **Database:** Redis (Deduplication), PostgreSQL (Metadata)
- **Search Engine:** Elasticsearch
- **Containerization:** Docker, Kubernetes (for deployment)

## ğŸ› ï¸ Setup & Installation
### **1ï¸âƒ£ Prerequisites**
- Java 17+
- Docker & Docker Compose
- Kafka & Zookeeper
- Elasticsearch & Redis

### **2ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/yourusername/web-crawler.git
cd web-crawler
```

### **3ï¸âƒ£ Run Services with Docker Compose**
```bash
docker-compose up -d
```

### **4ï¸âƒ£ Start Each Microservice**
Run each service separately using:
```bash
cd crawler-service && mvn spring-boot:run
cd parser-service && mvn spring-boot:run
cd extractor-service && mvn spring-boot:run
...
```

## ğŸš€ Usage
### **1ï¸âƒ£ Add a Seed URL to Start Crawling**
```bash
curl -X POST http://localhost:{urlservice's port} -d '['url']' -H "Content-Type: application/json"
```

### **2ï¸âƒ£ Search Indexed Pages**
```bash
curl -X GET 'http://localhost:9200/webpages/_search?q=your_keyword'
```

## ğŸ“Š Performance Optimizations
- **Asynchronous processing with RabbitMQ** for handling high throughput
- **Redis caching** for fast deduplication
- **Parallel crawling with multiple workers**
- **Batch indexing in Elasticsearch** for optimized search performance

## ğŸ“œ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

ğŸ’¡ **Contributions are welcome!** Feel free to submit PRs or issues. Happy Crawling! ğŸš€

